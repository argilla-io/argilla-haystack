{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haystack: Monitoring LLMs for Agents\n",
    "\n",
    "This guide will navigate you through utilizing ArgillaCallback to monitor the LLMS of your Haystack agents and seamlessly log the results into your Argilla Server. Haystack provides powerful tools for constructing and overseeing comprehensive pipelines for LLM production, enabling smooth collaboration with Conversational or RAG models, among others. The seamless integration with Argilla further simplifies the process of monitoring these pipelines. As a result, you gain the flexibility to effortlessly seek human feedback for assessing and optimizing your model's performance.\n",
    "\n",
    "In this tutorial, we will see two examples where we can benefit from ArgillaCallback, the first one being a simple example of logging into Argilla while the second one being a more advanced one where we will employ some tools for our agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use ArgillaCallback within your Haystack workflow, you first need to create an agent. For this tutorial, we will use a simple conversational agent that will be able to answer questions. We will also use GPT3.5 from OpenAI as our LLM. For this, you will need a valid API key from OpenAI. You can have more info and get one via [this link](https://openai.com/blog/openai-api).\n",
    "\n",
    "After you get your API key, let us import the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\", None) or getpass(\"Enter OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet below will create a simple conversational agent using Haystack `PromptNode`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptNode\n",
    "from haystack.agents.memory import ConversationSummaryMemory\n",
    "from haystack.agents.conversational import ConversationalAgent\n",
    "\n",
    "prompt_node = PromptNode(\n",
    "    model_name_or_path=\"gpt-3.5-turbo-instruct\", api_key=openai_api_key, max_length=256, stop_words=[\"Human\"]\n",
    ")\n",
    "summary_memory = ConversationSummaryMemory(prompt_node)\n",
    "conversational_agent = ConversationalAgent(prompt_node=prompt_node, memory=summary_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an up and running agent, we can employ the `ArgillaCallback`. To initialize the callback, you will need to provide the agent we have created and also the name of the dataset in the Argilla Server, where it will log in the results. Note that, if you do not have a dataset with the given name, it will create one for you. The created dataset will have the fields of `prompt` and `response` while also a `RatingQuestion` with values from 1 to 5.\n",
    "\n",
    "To properly connect to your server, you will also need to provide the api_key and the api_url of your server. If not provided, it will use the default values. Additionally, you have the opportunity to provide the workspace that you will specifically work within. In case it is not provided, again it will use the default workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla_haystack import ArgillaCallback\n",
    "\n",
    "api_key = \"argilla.apikey\"\n",
    "api_url = \"http://localhost:6900/\"\n",
    "dataset_name = \"conversational_ai\"\n",
    "\n",
    "ArgillaCallback(agent=conversational_agent, dataset_name=dataset_name, api_url=api_url, api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the agent defined and callback initialized, we are now ready to monitor our agent and log into Argilla. Let us run the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent conversational-agent-without-tools started with {'query': 'Tell me three most interesting things about Istanbul, Turkey', 'params': None}\n",
      "\u001b[32m Istanbul\u001b[0m\u001b[32m is\u001b[0m\u001b[32m a\u001b[0m\u001b[32m city\u001b[0m\u001b[32m with\u001b[0m\u001b[32m a\u001b[0m\u001b[32m rich\u001b[0m\u001b[32m history\u001b[0m\u001b[32m and culture, here are\u001b[0m\u001b[32m three interesting\u001b[0m\u001b[32m things\u001b[0m\u001b[32m about it\u001b[0m\u001b[32m:\n",
      "\n",
      "\u001b[0m\u001b[32m1.\u001b[0m\u001b[32m The Grand B\u001b[0m\u001b[32mazaar\u001b[0m\u001b[32m in\u001b[0m\u001b[32m Istanbul\u001b[0m\u001b[32m is one of the oldest and\u001b[0m\u001b[32m largest\u001b[0m\u001b[32m covered\u001b[0m\u001b[32m markets\u001b[0m\u001b[32m in\u001b[0m\u001b[32m the\u001b[0m\u001b[32m world\u001b[0m\u001b[32m,\u001b[0m\u001b[32m dating\u001b[0m\u001b[32m back\u001b[0m\u001b[32m to\u001b[0m\u001b[32m the\u001b[0m\u001b[32m \u001b[0m\u001b[32m15\u001b[0m\u001b[32mth\u001b[0m\u001b[32m century\u001b[0m\u001b[32m.\u001b[0m\u001b[32m It is a bustling hub\u001b[0m\u001b[32m of\u001b[0m\u001b[32m trade\u001b[0m\u001b[32m and\u001b[0m\u001b[32m commerce\u001b[0m\u001b[32m,\u001b[0m\u001b[32m with\u001b[0m\u001b[32m over\u001b[0m\u001b[32m \u001b[0m\u001b[32m4\u001b[0m\u001b[32m,\u001b[0m\u001b[32m000\u001b[0m\u001b[32m shops\u001b[0m\u001b[32m selling\u001b[0m\u001b[32m everything\u001b[0m\u001b[32m from\u001b[0m\u001b[32m spices\u001b[0m\u001b[32m to\u001b[0m\u001b[32m carpets\u001b[0m\u001b[32m.\n",
      "\n",
      "\u001b[0m\u001b[32m2\u001b[0m\u001b[32m.\u001b[0m\u001b[32m Istanbul\u001b[0m\u001b[32m is\u001b[0m\u001b[32m home\u001b[0m\u001b[32m to\u001b[0m\u001b[32m the\u001b[0m\u001b[32m largest\u001b[0m\u001b[32m palace\u001b[0m\u001b[32m in\u001b[0m\u001b[32m Europe\u001b[0m\u001b[32m,\u001b[0m\u001b[32m the\u001b[0m\u001b[32m Top\u001b[0m\u001b[32mk\u001b[0m\u001b[32mapi\u001b[0m\u001b[32m Palace\u001b[0m\u001b[32m.\u001b[0m\u001b[32m It\u001b[0m\u001b[32m was\u001b[0m\u001b[32m the\u001b[0m\u001b[32m residence\u001b[0m\u001b[32m of\u001b[0m\u001b[32m the\u001b[0m\u001b[32m Ottoman\u001b[0m\u001b[32m s\u001b[0m\u001b[32mult\u001b[0m\u001b[32mans\u001b[0m\u001b[32m for\u001b[0m\u001b[32m nearly\u001b[0m\u001b[32m \u001b[0m\u001b[32m400\u001b[0m\u001b[32m years\u001b[0m\u001b[32m and is now a\u001b[0m\u001b[32m popular tourist\u001b[0m\u001b[32m attraction\u001b[0m\u001b[32m.\n",
      "\n",
      "\u001b[0m\u001b[32m3.\u001b[0m\u001b[32m Istanbul has\u001b[0m\u001b[32m a\u001b[0m\u001b[32m unique\u001b[0m\u001b[32m and\u001b[0m\u001b[32m delicious\u001b[0m\u001b[32m cuisine\u001b[0m\u001b[32m that\u001b[0m\u001b[32m combines\u001b[0m\u001b[32m elements\u001b[0m\u001b[32m from\u001b[0m\u001b[32m both\u001b[0m\u001b[32m European and\u001b[0m\u001b[32m Middle\u001b[0m\u001b[32m Eastern\u001b[0m\u001b[32m cultures.\u001b[0m\u001b[32m Some popular dishes\u001b[0m\u001b[32m include\u001b[0m\u001b[32m ke\u001b[0m\u001b[32mb\u001b[0m\u001b[32mabs\u001b[0m\u001b[32m,\u001b[0m\u001b[32m bak\u001b[0m\u001b[32ml\u001b[0m\u001b[32mava\u001b[0m\u001b[32m,\u001b[0m\u001b[32m and\u001b[0m\u001b[32m Turkish\u001b[0m\u001b[32m delight\u001b[0m\u001b[32m.\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4265fe4a30490493055dcef9093253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/05/24 14:27:55] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> INFO:haystack_argilla.base:Records have been updated to Argilla            <a href=\"file:///Users/kursat/argilla-haystack/haystack_argilla/base.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kursat/argilla-haystack/haystack_argilla/base.py#214\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">214</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/05/24 14:27:55]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m INFO:haystack_argilla.base:Records have been updated to Argilla            \u001b]8;id=746317;file:///Users/kursat/argilla-haystack/haystack_argilla/base.py\u001b\\\u001b[2mbase.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=416370;file:///Users/kursat/argilla-haystack/haystack_argilla/base.py#214\u001b\\\u001b[2m214\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Tell me three most interesting things about Istanbul, Turkey',\n",
       " 'answers': [<Answer {'answer': 'Istanbul is a city with a rich history and culture, here are three interesting things about it:\\n\\n1. The Grand Bazaar in Istanbul is one of the oldest and largest covered markets in the world, dating back to the 15th century. It is a bustling hub of trade and commerce, with over 4,000 shops selling everything from spices to carpets.\\n\\n2. Istanbul is home to the largest palace in Europe, the Topkapi Palace. It was the residence of the Ottoman sultans for nearly 400 years and is now a popular tourist attraction.\\n\\n3. Istanbul has a unique and delicious cuisine that combines elements from both European and Middle Eastern cultures. Some popular dishes include kebabs, baklava, and Turkish delight.', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
       " 'transcript': ' Istanbul is a city with a rich history and culture, here are three interesting things about it:\\n\\n1. The Grand Bazaar in Istanbul is one of the oldest and largest covered markets in the world, dating back to the 15th century. It is a bustling hub of trade and commerce, with over 4,000 shops selling everything from spices to carpets.\\n\\n2. Istanbul is home to the largest palace in Europe, the Topkapi Palace. It was the residence of the Ottoman sultans for nearly 400 years and is now a popular tourist attraction.\\n\\n3. Istanbul has a unique and delicious cuisine that combines elements from both European and Middle Eastern cultures. Some popular dishes include kebabs, baklava, and Turkish delight.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent.run(\"Tell me three most interesting things about Istanbul, Turkey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, the agent responded to the query and gave us the response. The records have also been updated to the Argilla Server. Let us check it on the Argilla Server.\n",
    "\n",
    "![Argilla Dataset](argilla-dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Agent to Search Your Documents\n",
    "\n",
    "If you would like to employ your agent in a more advanced workflow, you can turn it to a decision-maker, where it determines the best course of action in a given situation. Haystac agents makes use of various Tools or Memory components to achieve a good performance where the task requires versatile LLM skills and each specific subtask needs to be handled differently. This tutorial is taken from the [Haystack documentation](https://haystack.deepset.ai/tutorials/25_customizing_agent) and you can find more tutorials and examples there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will again use GPT3.5 from OpenAI as our LLM. For this, you will need a valid API key from OpenAI. You can have more info and get one via [this link](https://openai.com/blog/openai-api).\n",
    "\n",
    "After you get your API key, let us import the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\", None) or getpass(\"Enter OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argilla_haystack import ArgillaCallback\n",
    "import argilla as rg\n",
    "from datasets import load_dataset\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack.nodes import PromptNode, PromptTemplate, AnswerParser, BM25Retriever\n",
    "from haystack.pipelines import Pipeline\n",
    "\n",
    "from haystack.agents import Tool\n",
    "from haystack.agents.memory import ConversationSummaryMemory\n",
    "from haystack.agents import AgentStep, Agent\n",
    "from haystack.agents.base import Agent, ToolsManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let us create the generative pipeline that our agent will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = InMemoryDocumentStore(use_bm25=True)\n",
    "document_store.write_documents(dataset)\n",
    "retriever = BM25Retriever(document_store=document_store)\n",
    "prompt0 = \"Please use a maximum of 50 tokens. Question: {query}\\nDocuments: {join(documents)}\\nAnswer:\"\n",
    "prompt_template = PromptTemplate(\n",
    "    prompt=prompt0,\n",
    "    output_parser=AnswerParser(),\n",
    ")\n",
    "prompt_node = PromptNode(\n",
    "    model_name_or_path=\"gpt-3.5-turbo-instruct\", api_key=openai_api_key, default_prompt_template=prompt_template\n",
    ")\n",
    "generative_pipeline = Pipeline()\n",
    "generative_pipeline.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "generative_pipeline.add_node(component=prompt_node, name=\"Prompt\", inputs=[\"Retriever\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the pipeline defined, let us create the tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = Tool(\n",
    "    name=\"Search_the_documents_tool\",\n",
    "    pipeline_or_node=generative_pipeline,\n",
    "    description=\"useful for when you need to answer questions about the seven wonders of the world\",\n",
    "    output_variable=\"answers\",\n",
    ")\n",
    "agent_prompt_node = PromptNode(\n",
    "    \"gpt-3.5-turbo\",\n",
    "    api_key=openai_api_key,\n",
    "    max_length=256,\n",
    "    stop_words=[\"Observation:\"],\n",
    "    model_kwargs={\"temperature\": 0.5},\n",
    ")\n",
    "memory_prompt_node = PromptNode(\n",
    "    model_name_or_path=\"philschmid/bart-large-cnn-samsum\", max_length=256, model_kwargs={\"task_name\":\"text2text-generation\"} \n",
    ")\n",
    "memory = ConversationSummaryMemory(\n",
    "    prompt_node=memory_prompt_node, prompt_template=\"{chat_transcript}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a prompt which will be used to query the tool by the agent. It should be sufficiently detailed to obtain a good response from the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_prompt = \"\"\"\n",
    "In the following conversation, a human user interacts with an AI Agent. The human user poses questions, and the AI Agent goes through several steps to provide well-informed answers.\n",
    "The AI Agent must use the available tools to find the up-to-date information. The final answer to the question should be truthfully based solely on the output of the tools. The AI Agent should ignore its knowledge when answering the questions.\n",
    "The AI Agent has access to these tools:\n",
    "{tool_names_with_descriptions}\n",
    "\n",
    "The following is the previous conversation between a human and The AI Agent:\n",
    "{memory}\n",
    "\n",
    "AI Agent responses must start with one of the following:\n",
    "\n",
    "Thought: [the AI Agent's reasoning process]\n",
    "Tool: [tool names] (on a new line) Tool Input: [input as a question for the selected tool WITHOUT quotation marks and on a new line] (These must always be provided together and on separate lines.)\n",
    "Observation: [tool's result]\n",
    "Final Answer: [final answer to the human user's question]\n",
    "\n",
    "When selecting a tool, the AI Agent must provide both the \"Tool:\" and \"Tool Input:\" pair in the same response, but on separate lines.\n",
    "\n",
    "The AI Agent should not ask the human user for additional information, clarification, or context.\n",
    "If the AI Agent cannot find a specific answer, it should accept the first word of the answer it found as the answer to the query.\n",
    "\n",
    "Question: {query}\n",
    "Thought:\n",
    "{transcript}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the last step before creating the agent, we need a resolver function that will handle the prompt given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolver_function(query, agent, agent_step):\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"tool_names_with_descriptions\": agent.tm.get_tool_names_with_descriptions(),\n",
    "        \"transcript\": agent_step.transcript,\n",
    "        \"memory\": agent.memory.load(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to define the agent with the given pipeline and prompt above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_agent = Agent(\n",
    "    agent_prompt_node,\n",
    "    prompt_template=agent_prompt,\n",
    "    prompt_parameters_resolver=resolver_function,\n",
    "    memory=memory,\n",
    "    tools_manager=ToolsManager([search_tool]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the agent ready, we will be initializing the `ArgillaCallback` as we did in the previous example. With the same credentiials, let us initialize the callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"argilla.apikey\"\n",
    "api_url = \"http://localhost:6900/\"\n",
    "dataset_name = \"search_the_documents\"\n",
    "\n",
    "ArgillaCallback(agent=conversational_agent, dataset_name=dataset_name, api_url=api_url, api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent custom-at-query-time started with {'query': 'Where is the temple of Artemis?', 'params': None}\n",
      "\u001b[32mThe\u001b[0m\u001b[32m temple\u001b[0m\u001b[32m of\u001b[0m\u001b[32m Artem\u001b[0m\u001b[32mis\u001b[0m\u001b[32m is\u001b[0m\u001b[32m one\u001b[0m\u001b[32m of\u001b[0m\u001b[32m the\u001b[0m\u001b[32m seven\u001b[0m\u001b[32m wonders\u001b[0m\u001b[32m of\u001b[0m\u001b[32m the\u001b[0m\u001b[32m ancient\u001b[0m\u001b[32m world\u001b[0m\u001b[32m.\u001b[0m\u001b[32m To\u001b[0m\u001b[32m find\u001b[0m\u001b[32m its\u001b[0m\u001b[32m location\u001b[0m\u001b[32m,\u001b[0m\u001b[32m I\u001b[0m\u001b[32m will\u001b[0m\u001b[32m use\u001b[0m\u001b[32m the\u001b[0m\u001b[32m Search\u001b[0m\u001b[32m_the\u001b[0m\u001b[32m_documents\u001b[0m\u001b[32m_tool\u001b[0m\u001b[32m.\u001b[0m\u001b[32m \n",
      "\u001b[0m\u001b[32mTool\u001b[0m\u001b[32m:\u001b[0m\u001b[32m Search\u001b[0m\u001b[32m_the\u001b[0m\u001b[32m_documents\u001b[0m\u001b[32m_tool\u001b[0m\u001b[32m\n",
      "\u001b[0m\u001b[32mTool\u001b[0m\u001b[32m Input\u001b[0m\u001b[32m:\u001b[0m\u001b[32m \"\u001b[0m\u001b[32mtem\u001b[0m\u001b[32mple\u001b[0m\u001b[32m of\u001b[0m\u001b[32m Artem\u001b[0m\u001b[32mis\u001b[0m\u001b[32m location\u001b[0m\u001b[32m\"\n",
      "\u001b[0mObservation: \u001b[33mThe Temple of Artemis was located near the ancient city of Ephesus, about 75 kilometres (47 mi) south from the modern port city of İzmir, in Turkey.\u001b[0m\n",
      "Thought: \u001b[32mThe\u001b[0m\u001b[32m Search\u001b[0m\u001b[32m_the\u001b[0m\u001b[32m_documents\u001b[0m\u001b[32m_tool\u001b[0m\u001b[32m provided\u001b[0m\u001b[32m the\u001b[0m\u001b[32m information\u001b[0m\u001b[32m that\u001b[0m\u001b[32m the\u001b[0m\u001b[32m Temple\u001b[0m\u001b[32m of\u001b[0m\u001b[32m Artem\u001b[0m\u001b[32mis\u001b[0m\u001b[32m was\u001b[0m\u001b[32m located\u001b[0m\u001b[32m near\u001b[0m\u001b[32m the\u001b[0m\u001b[32m ancient\u001b[0m\u001b[32m city\u001b[0m\u001b[32m of\u001b[0m\u001b[32m Eph\u001b[0m\u001b[32mesus\u001b[0m\u001b[32m,\u001b[0m\u001b[32m about\u001b[0m\u001b[32m \u001b[0m\u001b[32m75\u001b[0m\u001b[32m kilometers\u001b[0m\u001b[32m south\u001b[0m\u001b[32m from\u001b[0m\u001b[32m the\u001b[0m\u001b[32m modern\u001b[0m\u001b[32m port\u001b[0m\u001b[32m city\u001b[0m\u001b[32m of\u001b[0m\u001b[32m İ\u001b[0m\u001b[32mz\u001b[0m\u001b[32mmir\u001b[0m\u001b[32m,\u001b[0m\u001b[32m in\u001b[0m\u001b[32m Turkey\u001b[0m\u001b[32m.\n",
      "\u001b[0m\u001b[32mFinal\u001b[0m\u001b[32m Answer\u001b[0m\u001b[32m:\u001b[0m\u001b[32m The\u001b[0m\u001b[32m temple\u001b[0m\u001b[32m of\u001b[0m\u001b[32m Artem\u001b[0m\u001b[32mis\u001b[0m\u001b[32m is\u001b[0m\u001b[32m located\u001b[0m\u001b[32m in\u001b[0m\u001b[32m Turkey\u001b[0m\u001b[32m,\u001b[0m\u001b[32m near\u001b[0m\u001b[32m the\u001b[0m\u001b[32m ancient\u001b[0m\u001b[32m city\u001b[0m\u001b[32m of\u001b[0m\u001b[32m Eph\u001b[0m\u001b[32mesus\u001b[0m\u001b[32m.\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9957742bbfa0479e86800fa8ca27eb95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01/05/24 15:08:33] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> INFO:haystack_argilla.base:Records have been updated to Argilla            <a href=\"file:///Users/kursat/argilla-haystack/haystack_argilla/base.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/kursat/argilla-haystack/haystack_argilla/base.py#214\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">214</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[01/05/24 15:08:33]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m INFO:haystack_argilla.base:Records have been updated to Argilla            \u001b]8;id=165015;file:///Users/kursat/argilla-haystack/haystack_argilla/base.py\u001b\\\u001b[2mbase.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=798323;file:///Users/kursat/argilla-haystack/haystack_argilla/base.py#214\u001b\\\u001b[2m214\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Where is the temple of Artemis?',\n",
       " 'answers': [<Answer {'answer': 'The temple of Artemis is located in Turkey, near the ancient city of Ephesus.', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
       " 'transcript': 'The temple of Artemis is one of the seven wonders of the ancient world. To find its location, I will use the Search_the_documents_tool. \\nTool: Search_the_documents_tool\\nTool Input: \"temple of Artemis location\"\\nObservation: The Temple of Artemis was located near the ancient city of Ephesus, about 75 kilometres (47 mi) south from the modern port city of İzmir, in Turkey.\\nThought:The Search_the_documents_tool provided the information that the Temple of Artemis was located near the ancient city of Ephesus, about 75 kilometers south from the modern port city of İzmir, in Turkey.\\nFinal Answer: The temple of Artemis is located in Turkey, near the ancient city of Ephesus.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent.run(\"Where is the temple of Artemis?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as giving the response to the user, the agent has also logged the results into the Argilla Server. Let us check it on the Argilla Server.\n",
    "\n",
    "![Argilla Dataset 2](argilla-dataset-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ArgillaCallback` has also logged the name of the tool used and the thought process of the agent into the record as `metadata`. Let us see the metadata of the record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_output': 'The Search_the_documents_tool provided the information that the Temple of Artemis was located near the ancient city of Ephesus, about 75 kilometers south from the modern port city of İzmir, in Turkey.\\nFinal Answer: The temple of Artemis is located in Turkey, near the ancient city of Ephesus.',\n",
       " 'tool_name': 'Search_the_documents_tool'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = rg.FeedbackDataset.from_argilla(dataset_name)\n",
    "dataset[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, the tool we named as \"Search_the_documents_tool\" has been used by the agent and the thought process has been logged into the record. This data can be quite beneficial for further analysis and optimization of the agent according to your project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
