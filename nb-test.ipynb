{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing base\n"
     ]
    }
   ],
   "source": [
    "from haystack_argilla.base import ArgillaCallback\n",
    "import argilla as rg\n",
    "from datasets import load_dataset\n",
    "\n",
    "from haystack.agents import Tool\n",
    "from haystack.agents.memory import ConversationSummaryMemory\n",
    "from haystack.agents import AgentStep, Agent\n",
    "from haystack.agents.base import Agent, ToolsManager\n",
    "\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack.nodes import PromptNode, PromptTemplate, AnswerParser, BM25Retriever\n",
    "from haystack.pipelines import Pipeline\n",
    "from haystack.utils import print_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create basic DS\n",
    "\n",
    "ds = rg.FeedbackDataset.for_retrieval_augmented_generation(1)\n",
    "records = rg.FeedbackRecord(\n",
    "    fields={\n",
    "        \"query\": \"Why can camels survive long without water?\",\n",
    "        \"retrieved_document_1\": \"because they are camels\"\n",
    "    },\n",
    ")\n",
    "ds.add_records(records)\n",
    "ds.push_to_argilla(\"haystack_argilla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPEN AI KEY\n",
    "openai_api_key = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HAYSTACK \n",
    "# based on tutorial https://haystack.deepset.ai/tutorials/25_customizing_agent\n",
    "\n",
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "\n",
    "## CREATE GENERATIVE PIPELINE\n",
    "document_store = InMemoryDocumentStore(use_bm25=True)\n",
    "document_store.write_documents(dataset)\n",
    "retriever = BM25Retriever(document_store=document_store)\n",
    "prompt0 = \"Please use a maximum of 50 tokens. Question: {query}\\nDocuments: {join(documents)}\\nAnswer:\"\n",
    "prompt_template = PromptTemplate(\n",
    "    prompt=prompt0,\n",
    "    output_parser=AnswerParser(),\n",
    ")\n",
    "prompt_node = PromptNode(\n",
    "    model_name_or_path=\"gpt-3.5-turbo-instruct\", api_key=openai_api_key, default_prompt_template=prompt_template\n",
    ")\n",
    "generative_pipeline = Pipeline()\n",
    "generative_pipeline.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "generative_pipeline.add_node(component=prompt_node, name=\"Prompt\", inputs=[\"Retriever\"])\n",
    "\n",
    "## CREATE AGENT\n",
    "search_tool = Tool(\n",
    "    name=\"Search_the_documents_tool\",\n",
    "    pipeline_or_node=generative_pipeline,\n",
    "    description=\"useful for when you need to answer questions about the seven wonders of the world\",\n",
    "    output_variable=\"answers\",\n",
    ")\n",
    "\n",
    "search_tool.run(\"Who lived in a town in Lydia?\")\n",
    "\n",
    "agent_prompt_node = PromptNode(\n",
    "    \"gpt-3.5-turbo\",\n",
    "    api_key=openai_api_key,\n",
    "    max_length=256,\n",
    "    stop_words=[\"Observation:\"],\n",
    "    model_kwargs={\"temperature\": 0.5},\n",
    ")\n",
    "memory_prompt_node = PromptNode(\n",
    "    model_name_or_path=\"philschmid/bart-large-cnn-samsum\", max_length=256, model_kwargs={\"task_name\":\"text2text-generation\"}  #Â MODEL\n",
    ")\n",
    "memory = ConversationSummaryMemory(\n",
    "    prompt_node=memory_prompt_node, prompt_template=\"{chat_transcript}\"\n",
    ")\n",
    "agent_prompt = \"\"\"\n",
    "In the following conversation, a human user interacts with an AI Agent. The human user poses questions, and the AI Agent goes through several steps to provide well-informed answers.\n",
    "The AI Agent must use the available tools to find the up-to-date information. The final answer to the question should be truthfully based solely on the output of the tools. The AI Agent should ignore its knowledge when answering the questions.\n",
    "The AI Agent has access to these tools:\n",
    "{tool_names_with_descriptions}\n",
    "\n",
    "The following is the previous conversation between a human and The AI Agent:\n",
    "{memory}\n",
    "\n",
    "AI Agent responses must start with one of the following:\n",
    "\n",
    "Thought: [the AI Agent's reasoning process]\n",
    "Tool: [tool names] (on a new line) Tool Input: [input as a question for the selected tool WITHOUT quotation marks and on a new line] (These must always be provided together and on separate lines.)\n",
    "Observation: [tool's result]\n",
    "Final Answer: [final answer to the human user's question]\n",
    "\n",
    "When selecting a tool, the AI Agent must provide both the \"Tool:\" and \"Tool Input:\" pair in the same response, but on separate lines.\n",
    "\n",
    "The AI Agent should not ask the human user for additional information, clarification, or context.\n",
    "If the AI Agent cannot find a specific answer, it should accept the first word of the answer it found as the answer to the query.\n",
    "\n",
    "Question: {query}\n",
    "Thought:\n",
    "{transcript}\n",
    "\"\"\"\n",
    "\n",
    "def resolver_function(query, agent, agent_step):\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"tool_names_with_descriptions\": agent.tm.get_tool_names_with_descriptions(),\n",
    "        \"transcript\": agent_step.transcript,\n",
    "        \"memory\": agent.memory.load(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE AGENT\n",
    "\n",
    "conversational_agent = Agent(\n",
    "    agent_prompt_node,\n",
    "    prompt_template=agent_prompt,\n",
    "    prompt_parameters_resolver=resolver_function,\n",
    "    memory=memory,\n",
    "    tools_manager=ToolsManager([search_tool]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGILLA CALLBACK\n",
    "\n",
    "api_key = \"argilla.apikey\"\n",
    "api_url = \"http://localhost:6900/\"\n",
    "dataset_id = \"haystack_argilla\"\n",
    "\n",
    "ArgillaCallback(agent=conversational_agent, dataset_name=dataset_id, api_url=api_url, api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing base\n",
      "\n",
      "Agent custom-at-query-time started with {'query': 'What is another name for Artemis?', 'params': None}\n",
      "\u001b[32mThe\u001b[0m\u001b[32m human\u001b[0m\u001b[32m user\u001b[0m\u001b[32m is\u001b[0m\u001b[32m asking\u001b[0m\u001b[32m for\u001b[0m\u001b[32m another\u001b[0m\u001b[32m name\u001b[0m\u001b[32m for\u001b[0m\u001b[32m Artem\u001b[0m\u001b[32mis\u001b[0m\u001b[32m.\u001b[0m\u001b[32m Based\u001b[0m\u001b[32m on\u001b[0m\u001b[32m the\u001b[0m\u001b[32m previous\u001b[0m\u001b[32m information\u001b[0m\u001b[32m,\u001b[0m\u001b[32m the\u001b[0m\u001b[32m AI\u001b[0m\u001b[32m Agent\u001b[0m\u001b[32m knows\u001b[0m\u001b[32m that\u001b[0m\u001b[32m Artem\u001b[0m\u001b[32mis\u001b[0m\u001b[32m is\u001b[0m\u001b[32m also\u001b[0m\u001b[32m known\u001b[0m\u001b[32m as\u001b[0m\u001b[32m Diana\u001b[0m\u001b[32m in\u001b[0m\u001b[32m Roman\u001b[0m\u001b[32m mythology\u001b[0m\u001b[32m.\u001b[0m\u001b[32m To\u001b[0m\u001b[32m provide\u001b[0m\u001b[32m the\u001b[0m\u001b[32m answer\u001b[0m\u001b[32m,\u001b[0m\u001b[32m the\u001b[0m\u001b[32m AI\u001b[0m\u001b[32m Agent\u001b[0m\u001b[32m can\u001b[0m\u001b[32m use\u001b[0m\u001b[32m the\u001b[0m\u001b[32m '\u001b[0m\u001b[32mSearch\u001b[0m\u001b[32m_the\u001b[0m\u001b[32m_documents\u001b[0m\u001b[32m_tool\u001b[0m\u001b[32m'\u001b[0m\u001b[32m to\u001b[0m\u001b[32m confirm\u001b[0m\u001b[32m this\u001b[0m\u001b[32m information\u001b[0m\u001b[32m.\n",
      "\n",
      "\u001b[0m\u001b[32mTool\u001b[0m\u001b[32m:\u001b[0m\u001b[32m Search\u001b[0m\u001b[32m_the\u001b[0m\u001b[32m_documents\u001b[0m\u001b[32m_tool\u001b[0m\u001b[32m\n",
      "\u001b[0m\u001b[32mTool\u001b[0m\u001b[32m Input\u001b[0m\u001b[32m:\u001b[0m\u001b[32m What\u001b[0m\u001b[32m is\u001b[0m\u001b[32m another\u001b[0m\u001b[32m name\u001b[0m\u001b[32m for\u001b[0m\u001b[32m Artem\u001b[0m\u001b[32mis\u001b[0m\u001b[32m?\n",
      "\n",
      "\u001b[0mObservation: \u001b[33mDiana\u001b[0m\n",
      "Thought: \u001b[32mThe\u001b[0m\u001b[32m '\u001b[0m\u001b[32mSearch\u001b[0m\u001b[32m_the\u001b[0m\u001b[32m_documents\u001b[0m\u001b[32m_tool\u001b[0m\u001b[32m'\u001b[0m\u001b[32m provided\u001b[0m\u001b[32m the\u001b[0m\u001b[32m answer\u001b[0m\u001b[32m '\u001b[0m\u001b[32mD\u001b[0m\u001b[32miana\u001b[0m\u001b[32m'\u001b[0m\u001b[32m as\u001b[0m\u001b[32m another\u001b[0m\u001b[32m name\u001b[0m\u001b[32m for\u001b[0m\u001b[32m Artem\u001b[0m\u001b[32mis\u001b[0m\u001b[32m.\n",
      "\n",
      "\u001b[0m\u001b[32mFinal\u001b[0m\u001b[32m Answer\u001b[0m\u001b[32m:\u001b[0m\u001b[32m Diana\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kursat/anaconda3/envs/argilla/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:418: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "/Users/kursat/anaconda3/envs/argilla/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:437: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `2.0` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc4b9e6606041a7b15ad385a8dc6d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records have been updated to Argilla\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is another name for Artemis?',\n",
       " 'answers': [<Answer {'answer': 'Diana', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
       " 'transcript': \"The human user is asking for another name for Artemis. Based on the previous information, the AI Agent knows that Artemis is also known as Diana in Roman mythology. To provide the answer, the AI Agent can use the 'Search_the_documents_tool' to confirm this information.\\n\\nTool: Search_the_documents_tool\\nTool Input: What is another name for Artemis?\\nObservation: Diana\\nThought:The 'Search_the_documents_tool' provided the answer 'Diana' as another name for Artemis.\\n\\nFinal Answer: Diana\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_agent.run(query=\"What is another name for Artemis?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "argilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
